{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d1f78c2",
      "metadata": {
        "id": "2d1f78c2"
      },
      "source": [
        "### Uncomment the following blocks in order to install dependencies in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/anger232/learning-time-series-counterfactuals.git\n",
        "#%cd learning-time-series-counterfactuals/ "
      ],
      "metadata": {
        "id": "jLEt3b2YtZzX"
      },
      "id": "jLEt3b2YtZzX",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4940569a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4940569a",
        "outputId": "06990b54-5e0a-48a5-e233-3ad66575ce3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 KB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.4/981.4 KB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 KB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.9/257.9 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 KB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install deps\n",
        "!pip install -q -U tensor2tensor\n",
        "!pip install -q tensorflow matplotlib\n",
        "!pip install -q wildboar\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "MwnjpBvYPoIh",
        "outputId": "1c81ae19-7ea1-438b-b110-59c9c1a7d157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MwnjpBvYPoIh",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/learning-time-series-counterfactuals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e955a6e8",
      "metadata": {
        "id": "e955a6e8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./src')\n",
        "sys.path.append('./LIMESegment/Utils/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307ebbad",
      "metadata": {
        "id": "307ebbad"
      },
      "source": [
        "### Actual codes start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da78e0e8",
      "metadata": {
        "id": "da78e0e8"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "\n",
        "from _composite import ModifiedLatentCF\n",
        "from _vanilla import LatentCF\n",
        "from help_functions import (ResultWriter, conditional_pad, evaluate,\n",
        "                            find_best_alpha, find_best_lr, plot_graphs,\n",
        "                            reset_seeds, time_series_normalize,\n",
        "                            time_series_revert, upsample_minority,\n",
        "                            validity_score)\n",
        "from keras_models import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bdac5a53",
      "metadata": {
        "id": "bdac5a53"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "79751faf",
      "metadata": {
        "id": "79751faf"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logger.info(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}.\")\n",
        "RANDOM_STATE = 39\n",
        "\n",
        "DATASET_NAME = \"TwoLeadECG\"\n",
        "OUTPUT_FILENAME = \"twolead-outfile.csv\"\n",
        "result_writer = ResultWriter(file_name=OUTPUT_FILENAME, dataset_name=DATASET_NAME)\n",
        "logger.info(f\"Result writer is ready, writing to {OUTPUT_FILENAME}...\")\n",
        "result_writer.write_head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "255ed3dc",
      "metadata": {
        "id": "255ed3dc",
        "outputId": "0a693ce8-99d0-4ef5-cd84-30b46500d345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading ucr-v1.0-default.zip (231.83 MB)\n",
            "  |██████████████████████████████████████████████████| 231.83/231.83 MB\n"
          ]
        }
      ],
      "source": [
        "# 1. Load data\n",
        "X, y = load_dataset(DATASET_NAME, repository=\"wildboar/ucr\")\n",
        "\n",
        "pos = 1\n",
        "neg = 2\n",
        "# Convert positive and negative labels to 1 and 0\n",
        "pos_label, neg_label = 1, 0\n",
        "y_copy = y.copy()\n",
        "if pos != pos_label:\n",
        "    y_copy[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "if neg != neg_label:\n",
        "    y_copy[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_copy, test_size=0.2, random_state=RANDOM_STATE, stratify=y_copy) \n",
        "\n",
        "# Upsample the minority class\n",
        "y_train_copy = y_train.copy()\n",
        "X_train, y_train = upsample_minority(X_train, y_train, pos_label=pos_label, neg_label=neg_label)\n",
        "if y_train.shape != y_train_copy.shape:\n",
        "    logger.info(f\"Data upsampling performed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "398b9372",
      "metadata": {
        "id": "398b9372"
      },
      "outputs": [],
      "source": [
        "# ### 1.1 Normalization - fit scaler using training data \n",
        "n_training, n_timesteps = X_train.shape\n",
        "n_features = 1\n",
        "\n",
        "X_train_processed, trained_scaler = time_series_normalize(data=X_train, n_timesteps=n_timesteps)\n",
        "X_test_processed, _ = time_series_normalize(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "\n",
        "X_train_processed_padded, padding_size = conditional_pad(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad(X_test_processed) \n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "logger.info(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2a9f72b9",
      "metadata": {
        "id": "2a9f72b9"
      },
      "outputs": [],
      "source": [
        "y_train_classes = y_train\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7b163a",
      "metadata": {
        "id": "bf7b163a",
        "outputId": "e708e3df-a301-466b-8ce4-45558216e177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - 2s 27ms/step - loss: 0.2778 - val_loss: 0.0267\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0218 - val_loss: 0.0132\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0107 - val_loss: 0.0084\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0064 - val_loss: 0.0045\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0036 - val_loss: 0.0029\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0017 - val_loss: 0.0014\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0012 - val_loss: 9.3980e-04\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 7.9582e-04 - val_loss: 6.6508e-04\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 5.7274e-04 - val_loss: 4.8159e-04\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 4.3454e-04 - val_loss: 3.8096e-04\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 3.5708e-04 - val_loss: 3.2103e-04\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 3.0748e-04 - val_loss: 2.8267e-04\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 2.7305e-04 - val_loss: 2.5454e-04\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 2.5103e-04 - val_loss: 2.3928e-04\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 2.3256e-04 - val_loss: 2.3233e-04\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 2.1437e-04 - val_loss: 2.0120e-04\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 2.0072e-04 - val_loss: 1.9280e-04\n",
            "Epoch 19/50\n",
            "26/30 [=========================>....] - ETA: 0s - loss: 1.9139e-04"
          ]
        }
      ],
      "source": [
        "# ## 2. LatentCF models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "\n",
        "###############################################\n",
        "# ## 2.1 1dCNN autoencoder + 1dCNN classifier\n",
        "###############################################\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder(n_timesteps_padded, n_features)\n",
        "optimizer = keras.optimizers.legacy.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\") \n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "logger.info(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train_processed_padded, \n",
        "    X_train_processed_padded, \n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=True, \n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "logger.info(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85274cc7",
      "metadata": {
        "id": "85274cc7"
      },
      "outputs": [],
      "source": [
        "def Classifier(n_timesteps, n_features, n_output=2, n_conv_layers=1, add_dense_layer=True):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "    inputs = keras.Input(shape=(n_timesteps, n_features), dtype=\"float32\")\n",
        "    \n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128)(inputs)\n",
        "    else: \n",
        "        x = inputs\n",
        "    \n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(n_output, activation='softmax')(x)\n",
        "    classifier = keras.Model(inputs, outputs)\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b59010",
      "metadata": {
        "scrolled": true,
        "id": "81b59010"
      },
      "outputs": [],
      "source": [
        "shallow_cnn = True\n",
        "# ### 1dCNN classifier\n",
        "if shallow_cnn == True:\n",
        "    logger.info(f\"Check shallow_cnn argument={shallow_cnn}, use the shallow structure.\")\n",
        "    classifier = Classifier(n_timesteps_padded, n_features, n_conv_layers=1, add_dense_layer=True) # shallow CNN for small data size\n",
        "else:\n",
        "    classifier = Classifier(n_timesteps_padded, n_features, n_conv_layers=3, add_dense_layer=False) # deeper CNN layers for data with larger size\n",
        "\n",
        "optimizer = keras.optimizers.legacy.Adam(lr=0.0001)\n",
        "classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "logger.info(\"Training log for 1dCNN classifier:\")\n",
        "classifier_history = classifier.fit(X_train_processed_padded, \n",
        "        y_train, \n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        shuffle=True, \n",
        "        verbose=True, \n",
        "        validation_data=(X_test_processed_padded, y_test),\n",
        "        callbacks=[early_stopping_accuracy])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f69f78d",
      "metadata": {
        "id": "3f69f78d"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "# y_pred_classes = np.array([1 if pred > 0.5 else 0 for pred in y_pred])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "logger.info(f\"1dCNN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "        confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "        index=['True:pos', 'True:neg'], \n",
        "        columns=['Pred:pos', 'Pred:neg']\n",
        "    )\n",
        "confusion_matrix_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c159d8",
      "metadata": {
        "id": "e2c159d8"
      },
      "source": [
        "## Get LIME local weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ddba00",
      "metadata": {
        "id": "f7ddba00"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../LIMESegment/Utils/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523baa28",
      "metadata": {
        "id": "523baa28"
      },
      "outputs": [],
      "source": [
        "from explanations import LIMESegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59a1c92",
      "metadata": {
        "id": "f59a1c92"
      },
      "outputs": [],
      "source": [
        "idx = 2 # explained instance\n",
        "series = X_test_processed_padded[idx]\n",
        "print(y_pred_classes[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0618da3b",
      "metadata": {
        "id": "0618da3b"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# parameters\n",
        "ts: TS array of shape T x 1 where T is length of time series\n",
        "model: Trained model on dataset array of shape n x T x 1\n",
        "model_type: String indicating if classificaton model produces binary output \"class\" or probability output \"proba\", default \"class\"\n",
        "distance: Distance metric to be used by LIMESegment default is 'dtw'\n",
        "window_size: Window size to be used by NNSegment default is T/5\n",
        "cp: Number of change points to be determinded by NNSegment default is 3\n",
        "f: Frequency parameter for RBP default is T/10\n",
        "\"\"\"\n",
        "\n",
        "explanations = LIMESegment(series, classifier, model_type='proba', cp=10, window_size=10)\n",
        "explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188b89bb",
      "metadata": {
        "id": "188b89bb"
      },
      "outputs": [],
      "source": [
        "seg_imp, seg_idx = explanations\n",
        "total_len = len(series)\n",
        "seg_idx[-1] = total_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c277ac",
      "metadata": {
        "id": "56c277ac"
      },
      "outputs": [],
      "source": [
        "series.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22c925a",
      "metadata": {
        "id": "b22c925a"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "plt.plot(series, color='b', label='Explained instance')\n",
        "# plt.plot(np.mean(X_test[y_test_classes == 1], axis=0), color='green', label='Mean of other class')\n",
        "plt.legend(loc='lower left')\n",
        "\n",
        "for i in range(len(seg_idx)-1):\n",
        "    weight = seg_imp[i]\n",
        "    start = seg_idx[i]\n",
        "    end = seg_idx[i+1] \n",
        "    print(f'{start}, {end}: {weight}')\n",
        "    color = 'red' if weight < 0 else 'green' \n",
        "    plt.axvspan(start, end, color=color, alpha=abs(weight))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74485f03",
      "metadata": {
        "id": "74485f03"
      },
      "outputs": [],
      "source": [
        "check = np.where(seg_imp >= 0.01) # seg_imp >= 0.01 or 0.1\n",
        "check[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32813bc9",
      "metadata": {
        "id": "32813bc9"
      },
      "outputs": [],
      "source": [
        "weighted_steps = np.ones(total_len)\n",
        "\n",
        "for start_idx in check[0]:\n",
        "    weighted_steps[seg_idx[start_idx]: seg_idx[start_idx+1]] = 0\n",
        "weighted_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f56c9f",
      "metadata": {
        "id": "67f56c9f"
      },
      "outputs": [],
      "source": [
        "np.where(weighted_steps == 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0839f7a6",
      "metadata": {
        "id": "0839f7a6"
      },
      "source": [
        "### UPDATE: modified LIME segment (TODO, intergrate the updated version in LatentCF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efdd6b6c",
      "metadata": {
        "id": "efdd6b6c"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "import stumpy\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.utils import check_random_state\n",
        "from fastdtw import fastdtw\n",
        "import random\n",
        "from explanations import NNSegment, RBP\n",
        "\n",
        "def LIMESegment(example, model, model_type='class', distance='dtw', n=100, window_size=None, cp=None, f=None, random_state=None):\n",
        "    random_state = check_random_state(random_state)\n",
        "    if window_size is None:\n",
        "        window_size =int(example.shape[0]/5)\n",
        "    if cp is None:\n",
        "        cp = 3\n",
        "    if f is None: \n",
        "        f = int(example.shape[0]/10)\n",
        "    \n",
        "    cp_indexes = NNSegment(example.reshape(example.shape[0]), window_size, cp)\n",
        "    segment_indexes = [0] + cp_indexes + [-1]\n",
        "    generated_samples_interpretable = [random_state.binomial(1, 0.5, len(cp_indexes)+1) for _ in range(0,n)] #UPDATE HERE\n",
        "    \n",
        "    generated_samples_raw = RBP(generated_samples_interpretable, example, segment_indexes, f)\n",
        "    sample_predictions = model.predict(generated_samples_raw)\n",
        "    \n",
        "#     print(np.argmax(sample_predictions, axis=1))\n",
        "\n",
        "    if model_type == 'proba':\n",
        "        y_labels = np.argmax(sample_predictions, axis=1)\n",
        "    elif isinstance(model_type, int): #UPDATE HERE\n",
        "        y_labels = sample_predictions[:, model_type]\n",
        "    else:\n",
        "        y_labels = sample_predictions\n",
        "    \n",
        "    if distance == 'dtw':\n",
        "        distances = np.asarray([fastdtw(example, sample)[0] for sample in generated_samples_raw])\n",
        "        weights = np.exp(-(np.abs((distances - np.mean(distances))/np.std(distances)).reshape(n,)))\n",
        "    elif distance == 'euclidean':\n",
        "        distances = np.asarray([np.linalg.norm(np.ones(len(cp_indexes)+1)-x) for x in generated_samples_interpretable])\n",
        "        weights = np.exp(-(np.abs(distances**2/0.75*(len(segment_indexes)**2)).reshape(n,)))\n",
        "\n",
        "    clf = Ridge(random_state=random_state) #UPDATE HERE\n",
        "    clf.fit(generated_samples_interpretable, y_labels, weights)\n",
        "\n",
        "    return clf.coef_, segment_indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe9328d",
      "metadata": {
        "id": "5fe9328d"
      },
      "outputs": [],
      "source": [
        "explanations = LIMESegment(series, classifier, model_type=0, random_state=12, cp=10, window_size=10) # warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9dbb12d",
      "metadata": {
        "id": "f9dbb12d"
      },
      "source": [
        "## CF generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496d7ac5",
      "metadata": {
        "id": "496d7ac5"
      },
      "outputs": [],
      "source": [
        "from _composite import extract_encoder_decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99221796",
      "metadata": {
        "id": "99221796"
      },
      "outputs": [],
      "source": [
        "class ModifiedLatentCF:\n",
        "    \"\"\"Explanations by generating a counterfacutal sample in the latent space of\n",
        "    any autoencoder.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Latent-CF: A Simple Baseline for Reverse Counterfactual Explanation\n",
        "        Rachana Balasubramanian and Samuel Sharpe and Brian Barr and Jason Wittenbach and C. Bayan Brus\n",
        "        In Proceedings of the Conference on Neural Information Processing Systems, 2020\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(    \n",
        "        self, \n",
        "        probability=0.5, \n",
        "        *, \n",
        "        alpha=0.001, \n",
        "        tolerance=1e-6, \n",
        "        learning_rate=1e-3, \n",
        "        max_iter=100,\n",
        "        optimizer=None,\n",
        "        autoencoder=None,\n",
        "        only_encoder=None,\n",
        "        only_decoder=None,\n",
        "        validity_loss_weight=1.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        probability : float, optional\n",
        "            The desired probability assigned by the model\n",
        "\n",
        "        alpha : float, optional\n",
        "            The step size\n",
        "\n",
        "        tolerance : float, optional\n",
        "            The maximum difference between the desired and assigned probability\n",
        "\n",
        "        learning_rate : float, optional\n",
        "            The learning rate of the optimizer\n",
        "\n",
        "        max_iter : int, optional\n",
        "            The maximum number of iterations\n",
        "\n",
        "        autoencoder : int, optional\n",
        "            The autoencoder for the latent representation\n",
        "\n",
        "            - if None the sample is generated in the original space\n",
        "            - if given, the autoencoder is expected to have `k` decoder layer and `k`\n",
        "              encoding layers.\n",
        "        \"\"\"\n",
        "        self.optimizer_ = tf.optimizers.legacy.Adam(learning_rate=1e-4) if optimizer is None else optimizer\n",
        "        self.mse_loss_ = keras.losses.MeanSquaredError() \n",
        "#         self.mae_loss_ = keras.losses.MeanAbsoluteError() \n",
        "        self.alpha_ = tf.constant(alpha)\n",
        "        self.probability_ = tf.constant(probability)\n",
        "        self.tolerance_ = tf.constant(tolerance)\n",
        "        self.max_iter = max_iter\n",
        "        self.autoencoder = autoencoder\n",
        "        self.only_encoder = only_encoder\n",
        "        self.only_decoder = only_decoder\n",
        "        \n",
        "        # Weights of the different loss components\n",
        "        self.validity_weight = validity_loss_weight\n",
        "        self.proximity_weight = (1 - self.validity_weight)\n",
        "\n",
        "    def fit(self, model):\n",
        "        \"\"\"Fit a new counterfactual explainer to the model\n",
        "\n",
        "        Paramaters\n",
        "        ----------\n",
        "\n",
        "        model : keras.Model\n",
        "            The model\n",
        "        \"\"\"\n",
        "        if self.autoencoder:\n",
        "            encode_input, encode_output, decode_input, decode_output = extract_encoder_decoder(self.autoencoder)\n",
        "            self.decoder_ = keras.Model(inputs=decode_input, outputs=decode_output)\n",
        "            self.encoder_ = keras.Model(inputs=encode_input, outputs=encode_output)\n",
        "        elif self.only_encoder and self.only_decoder:\n",
        "            self.encoder_ = self.only_encoder\n",
        "            self.decoder_ = self.only_decoder\n",
        "        else:\n",
        "            self.decoder_ = None\n",
        "            self.encoder_ = None\n",
        "        self.model_ = model\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Compute the differnece beteween the desired and actual probability\n",
        "\n",
        "        Paramters\n",
        "        ---------\n",
        "        x : Variable\n",
        "            Variable of the sample\n",
        "        \"\"\"\n",
        "        if self.autoencoder is None:\n",
        "            z = x\n",
        "        else:\n",
        "            z = self.decoder_(x)\n",
        "        \n",
        "        return self.model_(z)\n",
        "    \n",
        "    # TODO: add global weights in the function input?\n",
        "    def get_local_weights(self, input_sample):\n",
        "        local_explanation = LIMESegment(input_sample, self.model_, model_type='proba', cp=10, window_size=10)\n",
        "        \n",
        "        check = np.where(seg_imp <= -0.01) # TODO: decide the threshold, 0.01 or 0.1? seg_imp >= threshold or or <= -threshold???\n",
        "        weighted_steps = np.ones(total_len) # if weights are all one? => same as a normal MAE function\n",
        "        for start_idx in check[0]:\n",
        "            weighted_steps[seg_idx[start_idx]: seg_idx[start_idx+1]] = 0\n",
        "        \n",
        "        weighted_steps = weighted_steps.reshape(1, n_timesteps_padded, 1) # need to reshape for multiplication in `tf.math.multiply()`\n",
        "        return weighted_steps\n",
        "    \n",
        "\n",
        "    # The \"validity_loss\" is designed to measure the prediction probability to the desired decision boundary\n",
        "    def validity_loss(self, prediction):\n",
        "        return self.mse_loss_(self.probability_, prediction) \n",
        "    \n",
        "    # An auxiliary MAE loss function to measure the proximity with step_weights\n",
        "    def weighted_mean_absolute_error(self, original_sample, cf_sample, step_weights):\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(tf.math.abs(original_sample - cf_sample), step_weights)\n",
        "        )\n",
        "    \n",
        "    def compute_loss(self, original_sample, z_search, step_weights): # additional input of step_weights\n",
        "        # Initialize the loss\n",
        "        loss = tf.zeros(shape=())\n",
        "        decoded = self.decoder_(z_search)\n",
        "        pred = self.model_(decoded)\n",
        "        \n",
        "        # Add validity_loss \n",
        "        valid_loss = self.validity_loss(pred)\n",
        "        loss += self.validity_weight * valid_loss\n",
        "\n",
        "        # Add proximity_loss\n",
        "        proxi_loss = self.weighted_mean_absolute_error(\n",
        "            original_sample, decoded, step_weights=tf.cast(step_weights, tf.float32)) \n",
        "        loss += self.proximity_weight * proxi_loss\n",
        "\n",
        "        return loss, valid_loss, proxi_loss\n",
        "\n",
        "    # TODO: compatible with the counterfactuals of wildboar\n",
        "    #       i.e., define the desired output target per label\n",
        "    def transform(self, x):\n",
        "        \"\"\"Generate counterfactual explanations\n",
        "\n",
        "        x : array-like of shape [n_samples, n_timestep, n_dims]\n",
        "            The samples\n",
        "        \"\"\"\n",
        "        if self.only_encoder is not None: # if only encoder, then return the latent embeddings\n",
        "            _, encoded_dim1, encoded_dim2 = self.only_encoder.layers[-1].output_shape\n",
        "            result_samples = np.empty((x.shape[0], encoded_dim1, encoded_dim2)) \n",
        "        else: \n",
        "            result_samples = np.empty(x.shape)\n",
        "\n",
        "        losses = np.empty(x.shape[0])\n",
        "        for i in range(x.shape[0]):\n",
        "            if i % 50 == 0: print(f'{i} samples been transformed.')\n",
        "            \n",
        "            step_weights = self.get_local_weights(x[i])\n",
        "            x_sample, loss = self._transform_sample_z(x[np.newaxis, i], step_weights)\n",
        "\n",
        "            result_samples[i] = x_sample\n",
        "            losses[i] = loss\n",
        "\n",
        "        return result_samples, losses\n",
        "\n",
        "    def _transform_sample_z(self, x, step_weights):\n",
        "        \"\"\"Generate counterfactual explanations\n",
        "\n",
        "        x : array-like of shape [n_samples, n_timestep, n_dims]\n",
        "            The samples\n",
        "        \"\"\"\n",
        "        # TODO: check_is_fitted(self)\n",
        "        if self.only_encoder or self.autoencoder is not None:\n",
        "            z = tf.Variable(self.encoder_(x))\n",
        "        else:\n",
        "            z = tf.Variable(x, dtype=tf.float32)\n",
        "\n",
        "        it = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, valid_loss, proxi_loss = self.compute_loss(x, z, step_weights)\n",
        "            \n",
        "        pred = self.model_(self.decoder_(z))\n",
        "        \n",
        "        # TODO: modify the loss to check both validity and proximity; how to design the condition here?\n",
        "        # while (valid_loss > self.tolerance_ or pred[:, 1] < self.probability_ or proxi_loss > 0.001)\n",
        "        while (valid_loss > self.tolerance_ or pred[:, 1] < self.probability_ ) and \\\n",
        "        (it < self.max_iter if self.max_iter else True):\n",
        "            # Get gradients of loss wrt the sample\n",
        "            grads = tape.gradient(loss, z)\n",
        "            # Update the weights of the sample\n",
        "            self.optimizer_.apply_gradients([(grads, z)])\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, valid_loss, proxi_loss = self.compute_loss(x, z, step_weights)\n",
        "            it += 1\n",
        "            pred = self.model_(self.decoder_(z))\n",
        "        \n",
        "        pred = self.model_(self.decoder_(z))\n",
        "        print(f'current valid: {valid_loss}, proxi: {proxi_loss}, pred prob:{pred}, iter: {it}.')\n",
        "  \n",
        "        return z.numpy() if self.autoencoder is None else self.decoder_(z).numpy(), float(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d35a59",
      "metadata": {
        "id": "65d35a59"
      },
      "outputs": [],
      "source": [
        "## Evaluation metrics\n",
        "# use radius to find the count of points - KDTree; a trained tree is needed for evaluation\n",
        "tree = KDTree(\n",
        "    X_train_processed[y_train_classes==pos_label].reshape(-1, n_timesteps), \n",
        "    leaf_size=40, metric='euclidean')\n",
        "max_distance = distance_matrix(\n",
        "    X_train_processed[y_train_classes==neg_label].reshape(-1, n_timesteps), \n",
        "    X_train_processed[y_train_classes==pos_label].reshape(-1, n_timesteps)).max()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21c3e55",
      "metadata": {
        "id": "e21c3e55"
      },
      "source": [
        "### loss weight: 0.01 valid + 0.99 proxi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec07ca78",
      "metadata": {
        "id": "ec07ca78"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5, autoencoder=autoencoder, optimizer=tf.optimizers.legacy.Adam(learning_rate=0.0001), \n",
        "    validity_loss_weight=0.01)\n",
        "\n",
        "cf_model.fit(classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea532aa4",
      "metadata": {
        "scrolled": true,
        "id": "ea532aa4"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_processed_padded[y_pred_classes == 0]) \n",
        "# y_pred\n",
        "\n",
        "X_pred_neg = X_test_processed_padded[y_pred_classes == 0]\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses = cf_model.transform(X_pred_neg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab892e00",
      "metadata": {
        "id": "ab892e00"
      },
      "outputs": [],
      "source": [
        "z_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "if padding_size != 0: # remove extra paddings after counterfactual generation\n",
        "    best_cf_samples = cf_embeddings[:, :-padding_size, :] \n",
        "    X_pred_neg = X_test_processed[y_pred_classes == neg_label] # use the unpadded X for evaluation\n",
        "\n",
        "evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "evaluate_res #  proximity, validity, cost_mean, cost_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d2b276",
      "metadata": {
        "id": "50d2b276"
      },
      "outputs": [],
      "source": [
        "# Map the results back to the original scale, for comparison\n",
        "actual_Xs = time_series_revert(X_pred_neg, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "actual_cfs = time_series_revert(best_cf_samples, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i in range(1, 5): # plot first 4 samples\n",
        "    ax = plt.subplot(3, 2, i)\n",
        "    idx = i-1 # the actual index of the sample is i-1\n",
        "    ax.set_title(\"prob(z) = %.2f, prob(x)=%.2f\" % (z_pred[idx], y_pred[:, 1][idx]))\n",
        "    ax.plot(actual_Xs[idx].reshape(-1), c=\"b\")\n",
        "    ax.plot(actual_cfs[idx].reshape(-1), c=\"r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c585862",
      "metadata": {
        "id": "1c585862"
      },
      "source": [
        "### loss weight: 0.25 valid + 0.75 proxi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a562cb6c",
      "metadata": {
        "id": "a562cb6c"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5, autoencoder=autoencoder, optimizer=tf.optimizers.legacy.Adam(learning_rate=0.0001), \n",
        "    validity_loss_weight=0.25)\n",
        "\n",
        "cf_model.fit(classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87bc5936",
      "metadata": {
        "scrolled": true,
        "id": "87bc5936"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_processed_padded[y_pred_classes == 0]) \n",
        "# y_pred\n",
        "\n",
        "X_pred_neg = X_test_processed_padded[y_pred_classes == 0]\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses = cf_model.transform(X_pred_neg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ac4b98",
      "metadata": {
        "id": "22ac4b98"
      },
      "outputs": [],
      "source": [
        "z_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "if padding_size != 0: # remove extra paddings after counterfactual generation\n",
        "    best_cf_samples = cf_embeddings[:, :-padding_size, :] \n",
        "    X_pred_neg = X_test_processed[y_pred_classes == neg_label] # use the unpadded X for evaluation\n",
        "\n",
        "evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "evaluate_res #  proximity, validity, cost_mean, cost_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1bdc3e",
      "metadata": {
        "id": "4b1bdc3e"
      },
      "outputs": [],
      "source": [
        "# Map the results back to the original scale, for comparison\n",
        "actual_Xs = time_series_revert(X_pred_neg, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "actual_cfs = time_series_revert(best_cf_samples, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i in range(1, 5): # plot first 4 samples\n",
        "    ax = plt.subplot(3, 2, i)\n",
        "    idx = i-1 # the actual index of the sample is i-1\n",
        "    ax.set_title(\"prob(z) = %.2f, prob(x)=%.2f\" % (z_pred[idx], y_pred[:, 1][idx]))\n",
        "    ax.plot(actual_Xs[idx].reshape(-1), c=\"b\")\n",
        "    ax.plot(actual_cfs[idx].reshape(-1), c=\"r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc58d89",
      "metadata": {
        "id": "5fc58d89"
      },
      "source": [
        "### loss weight: 0.5 valid + 0.5 proxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d43605",
      "metadata": {
        "id": "78d43605"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5, autoencoder=autoencoder, optimizer=tf.optimizers.legacy.Adam(learning_rate=0.0001), \n",
        "    validity_loss_weight=0.5)\n",
        "\n",
        "cf_model.fit(classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212f18fe",
      "metadata": {
        "scrolled": true,
        "id": "212f18fe"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_processed_padded[y_pred_classes == 0]) \n",
        "# y_pred\n",
        "\n",
        "X_pred_neg = X_test_processed_padded[y_pred_classes == 0]\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses = cf_model.transform(X_pred_neg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e29f240",
      "metadata": {
        "id": "7e29f240"
      },
      "outputs": [],
      "source": [
        "z_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "if padding_size != 0: # remove extra paddings after counterfactual generation\n",
        "    best_cf_samples = cf_embeddings[:, :-padding_size, :] \n",
        "    X_pred_neg = X_test_processed[y_pred_classes == neg_label] # use the unpadded X for evaluation\n",
        "\n",
        "evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "evaluate_res #  proximity, validity, cost_mean, cost_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23cd4f6",
      "metadata": {
        "id": "b23cd4f6"
      },
      "outputs": [],
      "source": [
        "# Map the results back to the original scale, for comparison\n",
        "actual_Xs = time_series_revert(X_pred_neg, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "actual_cfs = time_series_revert(best_cf_samples, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i in range(1, 5): # plot first 4 samples\n",
        "    ax = plt.subplot(3, 2, i)\n",
        "    idx = i-1 # the actual index of the sample is i-1\n",
        "    ax.set_title(\"prob(z) = %.2f, prob(x)=%.2f\" % (z_pred[idx], y_pred[:, 1][idx]))\n",
        "    ax.plot(actual_Xs[idx].reshape(-1), c=\"b\")\n",
        "    ax.plot(actual_cfs[idx].reshape(-1), c=\"r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae4b832",
      "metadata": {
        "id": "fae4b832"
      },
      "source": [
        "### loss weight: 0.75 valid + 0.25 proxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2d0d1a",
      "metadata": {
        "id": "fe2d0d1a"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5, autoencoder=autoencoder, optimizer=tf.optimizers.legacy.Adam(learning_rate=0.0001), \n",
        "    validity_loss_weight=0.75)\n",
        "\n",
        "cf_model.fit(classifier)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918280df",
      "metadata": {
        "id": "918280df"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_processed_padded[y_pred_classes == 0]) \n",
        "# y_pred\n",
        "\n",
        "X_pred_neg = X_test_processed_padded[y_pred_classes == 0]\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses = cf_model.transform(X_pred_neg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18a50bb",
      "metadata": {
        "id": "c18a50bb"
      },
      "outputs": [],
      "source": [
        "z_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "if padding_size != 0: # remove extra paddings after counterfactual generation\n",
        "    best_cf_samples = cf_embeddings[:, :-padding_size, :] \n",
        "    X_pred_neg = X_test_processed[y_pred_classes == neg_label] # use the unpadded X for evaluation\n",
        "\n",
        "evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "evaluate_res #  proximity, validity, cost_mean, cost_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73fc9c86",
      "metadata": {
        "id": "73fc9c86"
      },
      "outputs": [],
      "source": [
        "# Map the results back to the original scale, for comparison\n",
        "actual_Xs = time_series_revert(X_pred_neg, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "actual_cfs = time_series_revert(best_cf_samples, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i in range(1, 5): # plot first 4 samples\n",
        "    ax = plt.subplot(3, 2, i)\n",
        "    idx = i-1 # the actual index of the sample is i-1\n",
        "    ax.set_title(\"prob(z) = %.2f, prob(x)=%.2f\" % (z_pred[idx], y_pred[:, 1][idx]))\n",
        "    ax.plot(actual_Xs[idx].reshape(-1), c=\"b\")\n",
        "    ax.plot(actual_cfs[idx].reshape(-1), c=\"r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac37578f",
      "metadata": {
        "id": "ac37578f"
      },
      "source": [
        "### loss weight: 1 valid + 0 proxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a555c820",
      "metadata": {
        "id": "a555c820"
      },
      "outputs": [],
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5, autoencoder=autoencoder, optimizer=tf.optimizers.legacy.Adam(learning_rate=0.0001), \n",
        "    validity_loss_weight=1.0)\n",
        "\n",
        "cf_model.fit(classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c1ce75",
      "metadata": {
        "scrolled": true,
        "id": "49c1ce75"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_processed_padded[y_pred_classes == 0]) \n",
        "# y_pred\n",
        "\n",
        "X_pred_neg = X_test_processed_padded[y_pred_classes == 0]\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses = cf_model.transform(X_pred_neg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f6bf78",
      "metadata": {
        "id": "59f6bf78"
      },
      "outputs": [],
      "source": [
        "z_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "if padding_size != 0: # remove extra paddings after counterfactual generation\n",
        "    best_cf_samples = cf_embeddings[:, :-padding_size, :] \n",
        "    X_pred_neg = X_test_processed[y_pred_classes == neg_label] # use the unpadded X for evaluation\n",
        "\n",
        "evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "evaluate_res #  proximity, validity, cost_mean, cost_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c73320",
      "metadata": {
        "id": "57c73320"
      },
      "outputs": [],
      "source": [
        "# Map the results back to the original scale, for comparison\n",
        "actual_Xs = time_series_revert(X_pred_neg, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "actual_cfs = time_series_revert(best_cf_samples, n_timesteps=n_timesteps, scaler=trained_scaler)\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "for i in range(1, 5): # plot first 4 samples\n",
        "    ax = plt.subplot(3, 2, i)\n",
        "    idx = i-1 # the actual index of the sample is i-1\n",
        "    ax.set_title(\"prob(z) = %.2f, prob(x)=%.2f\" % (z_pred[idx], y_pred[:, 1][idx]))\n",
        "    ax.plot(actual_Xs[idx].reshape(-1), c=\"b\")\n",
        "    ax.plot(actual_cfs[idx].reshape(-1), c=\"r\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}